{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:40:15,568 - INFO - [__main__] - Added . to sys.path\n",
      "2025-04-01 15:40:19,765 - INFO - [utils] - Attempting to load configuration from: config.json\n",
      "2025-04-01 15:40:19,787 - INFO - [utils] - Configuration loaded successfully.\n",
      "2025-04-01 15:40:19,787 - INFO - [utils] - Checking and creating save directories specified in config...\n",
      "2025-04-01 15:40:19,789 - INFO - [tuning] - Loading preprocessed data for tuning...\n",
      "2025-04-01 15:40:19,789 - INFO - [utils] - --- Attempting to LOAD preprocessed data from .joblib files ---\n",
      "2025-04-01 15:40:20,030 - INFO - [utils] - Successfully loaded Processed Data from: d:\\Downloads\\StressProject\\outputs\\processed_data\\processed_aligned_data.joblib\n",
      "2025-04-01 15:40:20,042 - INFO - [utils] - Successfully loaded Static Features from: d:\\Downloads\\StressProject\\outputs\\static_features\\static_features_results.joblib\n",
      "2025-04-01 15:40:20,052 - INFO - [utils] - Successfully loaded R-Peak Indices from: d:\\Downloads\\StressProject\\outputs\\static_features\\r_peak_indices.joblib\n",
      "2025-04-01 15:40:20,054 - INFO - [tuning] - Preprocessed data loaded successfully for tuning.\n",
      "2025-04-01 15:40:20,056 - INFO - [utils] - Attempting to load configuration from: config.json\n",
      "2025-04-01 15:40:20,058 - INFO - [utils] - Configuration loaded successfully.\n",
      "2025-04-01 15:40:20,059 - INFO - [utils] - Checking and creating save directories specified in config...\n",
      "2025-04-01 15:40:20,061 - INFO - [__main__] - Configuration loaded successfully.\n",
      "2025-04-01 15:40:20,079 - INFO - [__main__] - Using device: cuda\n",
      "2025-04-01 15:40:20,088 - INFO - [__main__] - CUDA Device Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "\n",
    "# Import standard libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint # For pretty printing config\n",
    "\n",
    "# --- Configure Logging ---\n",
    "# Setup basic logging to see outputs from modules\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s')\n",
    "log = logging.getLogger(__name__) # Logger for the notebook itself\n",
    "\n",
    "# --- Add project directory to Python path (if needed) ---\n",
    "# Ensure Python can find your modules (utils.py, data_loader.py, etc.)\n",
    "# Adjust the path as necessary relative to your notebook location\n",
    "project_path = '.' # Assuming notebook is in the root project directory\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "    log.info(f\"Added {project_path} to sys.path\")\n",
    "\n",
    "# --- Import Project Modules ---\n",
    "try:\n",
    "    from utils import load_config, load_preprocessed_data, safe_get\n",
    "    from data_loader import load_all_datasets\n",
    "    # Import the main function from the updated preprocessing module\n",
    "    from preprocessing import preprocess_all_subjects\n",
    "    # Import the main function from the updated data_pipeline module\n",
    "    from data_pipeline import prepare_dataloaders\n",
    "    from models import get_model\n",
    "    from training import train_model\n",
    "    from evaluation import evaluate_model, find_best_threshold, calculate_shap_importance\n",
    "    from tuning import run_tuning # Optional: for hyperparameter tuning\n",
    "    # Import widget setup functions\n",
    "    from widget_setup import (\n",
    "        setup_raw_signal_plotter,\n",
    "        setup_comparison_plotter,\n",
    "        setup_hrv_plotter,\n",
    "        setup_prediction_plotter,\n",
    "        display_evaluation_results\n",
    "    )\n",
    "    # Import specific plotting functions if needed directly (usually called via widgets)\n",
    "    # from visualization import plot_training_history\n",
    "except ImportError as e:\n",
    "    log.critical(f\"Failed to import necessary project modules: {e}. Ensure modules are in the Python path.\")\n",
    "    # Stop execution or handle error appropriately\n",
    "\n",
    "# --- Load Configuration ---\n",
    "CONFIG_PATH = 'config.json' # Path to your config file\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "if config:\n",
    "    log.info(\"Configuration loaded successfully.\")\n",
    "    # Optional: Pretty print the loaded config\n",
    "    # print(\"--- Configuration ---\")\n",
    "    # pprint.pprint(config)\n",
    "    # print(\"-\" * 20)\n",
    "else:\n",
    "    log.critical(\"Failed to load configuration. Please check config.json path and format.\")\n",
    "    # Stop execution if config fails to load\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f\"Using device: {device}\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    log.info(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data using the data_loader module\n",
    "all_subject_data, subjects_loaded, subjects_failed = load_all_datasets(config)\n",
    "\n",
    "if not all_subject_data:\n",
    "    log.critical(\"No raw data loaded. Exiting.\")\n",
    "    # Handle error appropriately\n",
    "else:\n",
    "    log.info(f\"Loaded raw data for {len(subjects_loaded)} subjects.\")\n",
    "    if subjects_failed:\n",
    "        log.warning(f\"Failed to load data for: {subjects_failed}\")\n",
    "\n",
    "# Store the list of loaded subject IDs (original format, e.g., 2, 'NURSE_1')\n",
    "# This is needed for the widget setups\n",
    "loaded_subject_ids_orig = subjects_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the interactive plotter for raw signals\n",
    "# Pass the raw data dictionary and the list of original subject IDs\n",
    "if all_subject_data and loaded_subject_ids_orig:\n",
    "    setup_raw_signal_plotter(config, all_subject_data, loaded_subject_ids_orig)\n",
    "else:\n",
    "    print(\"Skipping raw signal plotter setup: Raw data or subject list missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "# Assuming other necessary imports like preprocess_all_subjects, load_preprocessed_data, safe_get are done elsewhere\n",
    "\n",
    "# Get logger instance (replace __name__ if needed)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# --- Assume config, all_subject_data, subjects_loaded are defined earlier ---\n",
    "# Example placeholders (replace with your actual variables):\n",
    "# config = load_config('config.json')\n",
    "# all_subject_data, subjects_loaded, _ = load_all_datasets(config)\n",
    "\n",
    "# --- Control Flag ---\n",
    "# Set this flag to True to load existing data, False to run preprocessing again.\n",
    "LOAD_SAVED_DATA = True # Or False\n",
    "\n",
    "# --- Initialize variables ---\n",
    "processed_data = None\n",
    "static_features_results = None\n",
    "r_peak_results = None\n",
    "\n",
    "# --- Conditional Preprocessing or Loading ---\n",
    "if LOAD_SAVED_DATA:\n",
    "    log.info(\"Attempting to load preprocessed data...\")\n",
    "    try:\n",
    "        # Ensure the function is available in the scope\n",
    "        from utils import load_preprocessed_data\n",
    "        processed_data, static_features_results, r_peak_results = load_preprocessed_data(config)\n",
    "        if not processed_data:\n",
    "            log.warning(\"load_preprocessed_data returned empty data. Preprocessing might need to be run.\")\n",
    "        else:\n",
    "            log.info(\"Successfully loaded preprocessed data.\")\n",
    "    except ImportError:\n",
    "         log.error(\"Could not import load_preprocessed_data from utils. Cannot load data.\")\n",
    "    except Exception as load_e:\n",
    "        log.error(f\"Error loading preprocessed data: {load_e}\", exc_info=True)\n",
    "        # Decide if you want to fallback to running preprocessing here, or just exit\n",
    "        # For now, it will just proceed and fail the final check if data is None\n",
    "\n",
    "else:\n",
    "    log.info(\"Running preprocessing pipeline (LOAD_SAVED_DATA is False)...\")\n",
    "    if 'all_subject_data' in locals() and 'subjects_loaded' in locals() and 'config' in locals():\n",
    "        try:\n",
    "            # Ensure the function is available in the scope\n",
    "            from preprocessing import preprocess_all_subjects\n",
    "            processed_data, static_features_results, r_peak_results = preprocess_all_subjects(\n",
    "                all_subject_data, subjects_loaded, config\n",
    "            )\n",
    "        except ImportError:\n",
    "            log.error(\"Could not import preprocess_all_subjects from preprocessing. Cannot run preprocessing.\")\n",
    "        except Exception as preprocess_e:\n",
    "            log.error(f\"Error running preprocessing pipeline: {preprocess_e}\", exc_info=True)\n",
    "            # processed_data will remain None\n",
    "    else:\n",
    "        log.error(\"Raw data ('all_subject_data', 'subjects_loaded') or 'config' not available. Cannot run preprocessing.\")\n",
    "\n",
    "\n",
    "# --- Final Check ---\n",
    "# This check runs regardless of whether loading or preprocessing was attempted\n",
    "if not processed_data:\n",
    "    log.critical(\"Preprocessing/loading failed or no processed data obtained. Exiting or handling error...\")\n",
    "    # Handle error appropriately (e.g., raise exception, exit script, etc.)\n",
    "    # exit() or raise RuntimeError(\"Failed to obtain processed data.\")\n",
    "else:\n",
    "    log.info(f\"Preprocessing complete or data loaded successfully for {len(processed_data)} subjects.\")\n",
    "    # Proceed with the rest of your pipeline using processed_data, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the comparison plotter (Raw vs. Resampled)\n",
    "if all_subject_data and processed_data and loaded_subject_ids_orig:\n",
    "    setup_comparison_plotter(config, all_subject_data, processed_data, loaded_subject_ids_orig)\n",
    "else:\n",
    "    print(\"Skipping comparison plotter setup: Raw or processed data missing.\")\n",
    "\n",
    "# Setup the ECG + R-Peaks + Labels plotter\n",
    "# It will try to load r_peak_results if not provided\n",
    "if processed_data:\n",
    "     setup_hrv_plotter(config, processed_data, r_peak_results) # Pass loaded r_peak_results if available\n",
    "else:\n",
    "     print(\"Skipping ECG plotter setup: Processed data missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_data and static_features_results:\n",
    "    train_loader, val_loader, test_loader, input_dim_sequence, input_dim_static = prepare_dataloaders(\n",
    "        processed_data, static_features_results, config\n",
    "    )\n",
    "    if train_loader and val_loader and test_loader:\n",
    "        log.info(\"DataLoaders created successfully.\")\n",
    "        log.info(f\"Input Dimensions - Sequence: {input_dim_sequence}, Static: {input_dim_static}\")\n",
    "    else:\n",
    "        log.critical(\"Failed to create DataLoaders. Cannot proceed with training.\")\n",
    "        # Handle error\n",
    "else:\n",
    "    log.critical(\"Processed data or static features missing. Cannot create DataLoaders.\")\n",
    "    # Handle error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dimensions were calculated successfully in the previous step\n",
    "if 'input_dim_sequence' in locals() and 'input_dim_static' in locals():\n",
    "    model = get_model(config, input_dim_sequence, input_dim_static)\n",
    "    log.info(f\"Model '{type(model).__name__}' built.\")\n",
    "    # Optional: Print model summary\n",
    "    # print(model)\n",
    "else:\n",
    "    log.critical(\"Input dimensions not available. Cannot build model.\")\n",
    "    # Handle error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m best_model_state, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir_models\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass directory to save best model\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# --- Plot Training History ---\u001b[39;00m\n",
      "File \u001b[1;32md:\\Downloads\\StressProject\\training.py:453\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, config, device, output_dir)\u001b[0m\n\u001b[0;32m    450\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# Validate one epoch (if val_loader exists)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Downloads\\StressProject\\training.py:225\u001b[0m, in \u001b[0;36m_train_one_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device, epoch_num, total_epochs)\u001b[0m\n\u001b[0;32m    221\u001b[0m         log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Optimizer step error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;66;03m# Continue to next batch even if step fails for one batch? Or stop? Continuing for now.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Accumulate loss (weighted by batch size for accurate averaging)\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m    226\u001b[0m     num_samples_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# --- Epoch End ---\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "# Assuming other necessary imports like train_model, safe_get, plot_training_history are done elsewhere\n",
    "\n",
    "# Get logger instance (replace __name__ if needed)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# --- Assume config, model, train_loader, val_loader, device are defined earlier ---\n",
    "# Example placeholders (replace with your actual variables):\n",
    "# config = load_config('config.json')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = get_model(config, input_dim_sequence, input_dim_static).to(device)\n",
    "# train_loader, val_loader, test_loader, _, _ = prepare_dataloaders(...)\n",
    "\n",
    "# Ensure model and loaders are available\n",
    "if 'model' in locals() and 'train_loader' in locals() and 'val_loader' in locals() and 'config' in locals() and 'device' in locals():\n",
    "    output_dir_models = safe_get(config, ['save_paths', 'models'])\n",
    "    if not output_dir_models:\n",
    "        log.warning(\"Model output directory not specified in config. Best model will not be saved to file.\")\n",
    "\n",
    "    log.info(\"Starting model training...\")\n",
    "    # Run training\n",
    "    best_model_state, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        output_dir=output_dir_models # Pass directory to save best model\n",
    "    )\n",
    "    log.info(\"Model training finished.\")\n",
    "\n",
    "    # --- Plot Training History ---\n",
    "    if history:\n",
    "        output_dir_results = safe_get(config, ['save_paths', 'results'])\n",
    "        if output_dir_results:\n",
    "            try:\n",
    "                # Ensure the function is available in the scope\n",
    "                from visualization import plot_training_history\n",
    "                plot_training_history(history, output_dir_results)\n",
    "            except ImportError:\n",
    "                 log.error(\"Could not import plot_training_history from visualization.\")\n",
    "            except Exception as plot_e:\n",
    "                 log.error(f\"Error plotting training history: {plot_e}\", exc_info=True)\n",
    "        else:\n",
    "            log.warning(\"Results directory not specified. Cannot save training history plot.\")\n",
    "    elif best_model_state is not None:\n",
    "         log.warning(\"Training completed but history dictionary was not returned.\")\n",
    "    else:\n",
    "        # This case implies train_model returned (None, None), indicating failure\n",
    "        log.error(\"Training failed or history was not returned.\")\n",
    "\n",
    "\n",
    "    # --- Clear CUDA Cache (if CUDA was used) ---\n",
    "    # Important after training, especially if followed by other GPU tasks or loops\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        try:\n",
    "            log.info(\"Clearing CUDA cache...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            log.info(\"CUDA cache cleared.\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Failed to clear CUDA cache: {e}\")\n",
    "\n",
    "    # --- Optional: Clean up variables to free memory ---\n",
    "    # Useful in notebooks or long-running scripts\n",
    "    # del model, train_loader, val_loader, history, best_model_state\n",
    "    # log.debug(\"Cleaned up training variables.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    log.critical(\"Model, DataLoaders, Config, or Device not available. Skipping training and cache clearing.\")\n",
    "    # Handle error appropriately (e.g., raise exception, exit script)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Ensure the best model state and test loader are available\n",
    "if 'best_model_state' in locals() and best_model_state and 'test_loader' in locals() and test_loader:\n",
    "    log.info(\"Evaluating the best model on the test set...\")\n",
    "    # Load the best model state into the model structure\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Get loss function (needed for reporting loss during evaluation)\n",
    "    # Note: pos_weight calculated earlier might be based on the *sampled* train set.\n",
    "    # For consistent loss reporting on the test set, usually use unweighted BCE or FocalLoss.\n",
    "    from losses import FocalLoss # Ensure import\n",
    "    eval_criterion = None\n",
    "    eval_loss_type = safe_get(config, ['training_config', 'loss_function'], 'bce').lower()\n",
    "    if eval_loss_type == 'focal':\n",
    "        alpha = safe_get(config, ['training_config', 'focal_loss_alpha'], 0.25)\n",
    "        gamma = safe_get(config, ['training_config', 'focal_loss_gamma'], 2.0)\n",
    "        eval_criterion = FocalLoss(alpha=alpha, gamma=gamma, reduction='mean')\n",
    "    else: # Default to unweighted BCE for evaluation reporting\n",
    "        eval_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    log.info(f\"Using {type(eval_criterion).__name__} for reporting evaluation loss.\")\n",
    "\n",
    "    output_dir_results = safe_get(config, ['save_paths', 'results'])\n",
    "    if not output_dir_results:\n",
    "        log.error(\"Results output directory not specified. Cannot save evaluation results/plots.\")\n",
    "        # Handle error or proceed without saving\n",
    "\n",
    "    # Run evaluation\n",
    "    test_results = evaluate_model(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        criterion=eval_criterion,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        output_dir=output_dir_results, # Pass directory to save plots\n",
    "        set_name=\"Test\",\n",
    "        threshold=None # Set to None to find best F1 threshold on test set, or specify e.g., 0.5\n",
    "    )\n",
    "\n",
    "    # Save numerical results to JSON\n",
    "    if test_results and output_dir_results:\n",
    "        results_save_path = os.path.join(output_dir_results, \"test_evaluation_results.json\")\n",
    "        try:\n",
    "            # Remove potentially large items before saving if desired\n",
    "            # test_results.pop('probabilities', None)\n",
    "            # test_results.pop('labels', None)\n",
    "            with open(results_save_path, 'w') as f:\n",
    "                json.dump(test_results, f, indent=4)\n",
    "            log.info(f\"Test evaluation results saved to {results_save_path}\")\n",
    "\n",
    "            # Display results and plots using the widget setup function\n",
    "            display_evaluation_results(config, results_save_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"Failed to save or display test evaluation results: {e}\")\n",
    "\n",
    "    elif not test_results:\n",
    "        log.error(\"Model evaluation failed.\")\n",
    "\n",
    "else:\n",
    "    log.critical(\"Best model state or test loader not available. Skipping evaluation.\")\n",
    "    # Handle error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires generating predictions for all windows first.\n",
    "# The evaluate_model function returns probabilities and labels, but not mapped back to subjects easily.\n",
    "# You might need a separate function to run inference on the test_loader (or val_loader)\n",
    "# and store predictions per subject ID and window start time.\n",
    "\n",
    "# --- Placeholder: Function to get predictions per subject ---\n",
    "def get_all_predictions(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    predictions_by_subject = {} # {subj_id: ([starts], [preds])}\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            seq, static, _, subj_ids_tensor, starts_tensor = batch_data # Get subj_ids and starts\n",
    "            seq = seq.to(device)\n",
    "            static = static.to(device) if hasattr(model, 'input_dim_static') and model.input_dim_static > 0 else None\n",
    "            outputs = model(seq, static)\n",
    "            probs = torch.sigmoid(outputs.squeeze())\n",
    "            preds = (probs > threshold).int().cpu().numpy()\n",
    "            starts = starts_tensor.cpu().numpy()\n",
    "            # Assuming subject IDs are stored in dataset.subject_ids_list if they were strings\n",
    "            # This part needs careful handling depending on how string IDs are managed in the Dataset\n",
    "            subj_ids_list = [dataloader.dataset.subject_ids_list[i] for i in range(len(seq))] # Example access\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                subj_id = subj_ids_list[i] # Get original subject ID\n",
    "                start = starts[i]\n",
    "                pred = preds[i]\n",
    "                if subj_id not in predictions_by_subject:\n",
    "                    predictions_by_subject[subj_id] = ([], [])\n",
    "                predictions_by_subject[subj_id][0].append(start)\n",
    "                predictions_by_subject[subj_id][1].append(pred)\n",
    "    return predictions_by_subject\n",
    "# --- End Placeholder ---\n",
    "\n",
    "# --- Generate predictions (e.g., on test set) ---\n",
    "if 'model' in locals() and 'test_loader' in locals() and test_loader:\n",
    "    log.info(\"Generating predictions for visualization...\")\n",
    "    # Use the threshold determined during evaluation or a fixed one (e.g., 0.5)\n",
    "    eval_threshold = test_results.get('threshold_used', 0.5) if 'test_results' in locals() and test_results else 0.5\n",
    "    all_predictions_map = get_all_predictions(model, test_loader, device, threshold=eval_threshold)\n",
    "\n",
    "    # --- Setup Prediction Plotter ---\n",
    "    if processed_data and all_predictions_map:\n",
    "        setup_prediction_plotter(config, processed_data, all_predictions_map)\n",
    "    else:\n",
    "        print(\"Skipping prediction plotter: Processed data or predictions map missing.\")\n",
    "else:\n",
    "    print(\"Skipping prediction plotter: Model or test loader not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Optuna Tuning ---\n",
    "# Set the number of trials\n",
    "num_tuning_trials = 50 # Example: 50 trials\n",
    "log.info(f\"Starting hyperparameter tuning for {num_tuning_trials} trials...\")\n",
    "run_tuning(n_trials=num_tuning_trials)\n",
    "log.info(\"Hyperparameter tuning finished.\")\n",
    "# # After tuning, check the 'best_hyperparameters.json' file saved in the results directory\n",
    "# # and update your main 'config.json' accordingly before final training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
